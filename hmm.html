<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>1. Hidden Markov Models &mdash; hmmlearn 0.1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/js/copybutton.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="hmmlearn 0.1.0 documentation" href="index.html" />
    <link rel="next" title="2. Reference" href="classes.html" />
    <link rel="prev" title="hmmlearn" href="index.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="classes.html" title="2. Reference"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="index.html" title="hmmlearn"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">hmmlearn 0.1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="hidden-markov-models">
<span id="hmm"></span><h1>1. Hidden Markov Models<a class="headerlink" href="#hidden-markov-models" title="Permalink to this headline">¶</a></h1>
<p><tt class="docutils literal"><span class="pre">hmmlearn</span></tt> implements the Hidden Markov Models (HMMs).
The HMM is a generative probabilistic model, in which a sequence of observable
<span class="math">\(\mathbf{X}\)</span> variable is generated by a sequence of internal hidden
state <span class="math">\(\mathbf{Z}\)</span>. The hidden states can not be observed directly.
The transitions between hidden states are assumed to have the form of a
(first-order) Markov chain. They can be specified by the start probability
vector <span class="math">\(\boldsymbol{\Pi}\)</span> and a transition probability matrix
<span class="math">\(\mathbf{A}\)</span>.
The emission probability of an observable can be any distribution with
parameters <span class="math">\(\boldsymbol{{\Theta}_i}\)</span>
conditioned on the current hidden state (e.g. multinomial, Gaussian).
The HMM is completely determined by
<span class="math">\(\boldsymbol{\Pi, \mathbf{A}}\)</span> and <span class="math">\(\boldsymbol{{\Theta}_i}\)</span>.</p>
<p>There are three fundamental problems for HMMs:</p>
<ul class="simple">
<li>Given the model parameters and observed data, estimate the optimal
sequence of hidden states.</li>
<li>Given the model parameters and observed data, calculate the likelihood
of the data.</li>
<li>Given just the observed data, estimate the model parameters.</li>
</ul>
<p>The first and the second problem can be solved by the dynamic programming
algorithms known as
the Viterbi algorithm and the Forward-Backward algorithm, respectively.
The last one can be solved by an iterative Expectation-Maximization (EM)
algorithm, known as the Baum-Welch algorithm.</p>
<p>See the ref listed below for further detailed information.</p>
<div class="topic">
<p class="topic-title first">References:</p>
<p>[Rabiner89] <a class="reference external" href="http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf">A tutorial on hidden Markov models and selected applications in speech recognition</a>
Lawrence, R. Rabiner, 1989</p>
</div>
<div class="section" id="using-hmm">
<h2>1.1. Using HMM<a class="headerlink" href="#using-hmm" title="Permalink to this headline">¶</a></h2>
<p>Classes in this module include <a class="reference internal" href="generated/hmmlearn.hmm.MultinomialHMM.html#hmmlearn.hmm.MultinomialHMM" title="hmmlearn.hmm.MultinomialHMM"><tt class="xref py py-class docutils literal"><span class="pre">MultinomialHMM</span></tt></a>, <a class="reference internal" href="generated/hmmlearn.hmm.GaussianHMM.html#hmmlearn.hmm.GaussianHMM" title="hmmlearn.hmm.GaussianHMM"><tt class="xref py py-class docutils literal"><span class="pre">GaussianHMM</span></tt></a>,
and <a class="reference internal" href="generated/hmmlearn.hmm.GMMHMM.html#hmmlearn.hmm.GMMHMM" title="hmmlearn.hmm.GMMHMM"><tt class="xref py py-class docutils literal"><span class="pre">GMMHMM</span></tt></a>. They implement HMM with emission probabilities
determined by multimomial distributions, Gaussian distributions
and mixtures of Gaussian distributions.</p>
<div class="section" id="building-hmm-and-generating-samples">
<h3>1.1.1. Building HMM and generating samples<a class="headerlink" href="#building-hmm-and-generating-samples" title="Permalink to this headline">¶</a></h3>
<p>You can build an HMM instance by passing the parameters described above to the
constructor. Then, you can generate samples from the HMM by calling <cite>sample</cite>.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">hmmlearn</span> <span class="kn">import</span> <span class="n">hmm</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">startprob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transmat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">covars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">GaussianHMM</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s">&quot;full&quot;</span><span class="p">,</span> <span class="n">startprob</span><span class="p">,</span> <span class="n">transmat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">means_</span> <span class="o">=</span> <span class="n">means</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">covars_</span> <span class="o">=</span> <span class="n">covars</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="auto_examples/plot_hmm_sampling.html#example-plot-hmm-sampling-py"><em>Demonstration of sampling from HMM</em></a></li>
</ul>
</div>
</div>
<div class="section" id="training-hmm-parameters-and-inferring-the-hidden-states">
<h3>1.1.2. Training HMM parameters and inferring the hidden states<a class="headerlink" href="#training-hmm-parameters-and-inferring-the-hidden-states" title="Permalink to this headline">¶</a></h3>
<p>You can train an HMM by calling the <cite>fit</cite> method. The input is &#8220;the list&#8221; of
the sequence of observed value. Note, since the EM algorithm is a gradient-based
optimization method, it will generally get stuck in local optima. You should try
to run <cite>fit</cite> with various initializations and select the highest scored model.
The score of the model can be calculated by the <cite>score</cite> method.
The inferred optimal hidden states can be obtained by calling <cite>predict</cite> method.
The <cite>predict</cite> method can be specified with decoder algorithm.
Currently the Viterbi algorithm (<cite>viterbi</cite>), and maximum a posteriori
estimation (<cite>map</cite>) are supported.
This time, the input is a single sequence of observed values.  Note, the states
in model2 will have a different order than those in the generating model.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">model2</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">GaussianHMM</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s">&quot;full&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">X</span><span class="p">])</span> 
<span class="go">GaussianHMM(algorithm=&#39;viterbi&#39;,...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Z2</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="auto_examples/plot_hmm_stock_analysis.html#example-plot-hmm-stock-analysis-py"><em>Gaussian HMM of stock data</em></a></li>
</ul>
</div>
</div>
<div class="section" id="implementing-hmms-with-custom-emission-probabilities">
<h3>1.1.3. Implementing HMMs with custom emission probabilities<a class="headerlink" href="#implementing-hmms-with-custom-emission-probabilities" title="Permalink to this headline">¶</a></h3>
<p>If you want to implement other emission probability (e.g. Poisson), you have to
implement a new HMM class by inheriting the <tt class="xref py py-class docutils literal"><span class="pre">_BaseHMM</span></tt> and overriding
the methods <cite>__init__</cite>, <cite>_compute_log_likelihood</cite>,
<cite>_set</cite> and <cite>_get</cite> for additional parameters,
<cite>_initialize_sufficient_statistics</cite>, <cite>_accumulate_sufficient_statistics</cite> and
<cite>_do_mstep</cite>.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">1. Hidden Markov Models</a><ul>
<li><a class="reference internal" href="#using-hmm">1.1. Using HMM</a><ul>
<li><a class="reference internal" href="#building-hmm-and-generating-samples">1.1.1. Building HMM and generating samples</a></li>
<li><a class="reference internal" href="#training-hmm-parameters-and-inferring-the-hidden-states">1.1.2. Training HMM parameters and inferring the hidden states</a></li>
<li><a class="reference internal" href="#implementing-hmms-with-custom-emission-probabilities">1.1.3. Implementing HMMs with custom emission probabilities</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="index.html"
                        title="previous chapter">hmmlearn</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="classes.html"
                        title="next chapter">2. Reference</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/hmm.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="classes.html" title="2. Reference"
             >next</a></li>
        <li class="right" >
          <a href="index.html" title="hmmlearn"
             >previous</a> |</li>
        <li><a href="index.html">hmmlearn 0.1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010 - 2015, hmmlearn developers (BSD License).
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>